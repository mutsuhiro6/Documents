# Basics of Spark RDDs

## Resilient Distributed Datasets (RDDs)

### Combinators on RDDs

```scala
map
flatMap
reduce
fold
aggregate
```

### Code Example

Given, `val encyclopedia: RDD[String]`, count the number of pages that mention “EPFL”.

```scala
val result = encyclopedia.filter(page => page.contains("EPFL")).count()
```

### “Hello World”

```scala
val rdd: RDD[String] = spark.textFile("hdfs://...")
val count = rdd.flatMap(line => line.split(" ")) //RDD[String]
			   .map(word => (word, 1))
			   .reduceByKey(_ + _)
```

- Line 2: separate lines into words
- Line 3: include something to count
- Line 4: sum up the value in the pairs

  #### `reduceByKey`

同じkeyのvalueを結合する．

```scala
reduceByKey(func: (V, V) => V): RDD[(K, V)]
```

### How to Create an RDD?

From a `SparkContext` (or `SparkSession`) object.

- `parallelize`: convert a local Scala collection to an RDD.
- `textFile`: read a text file from HDFS or a local file system and return an RDD of `String`

## Transformation and Actions

*Actions:* operations on RDDs

### [RECALL] Scala’s transformers and accesors

#### Transformers

**Examples:** `map`, `filter`, `flatMap`, `groupBy`

```scala
map(f: A => B): Traversable[B]
```

Return new collections as results. (Not single values.)

#### Accessors

**Examples: ** `reduce`, `fold`, `aggregate`

```scala
reduce(op: (A, A) => A): A
```

Return single values as results. (Not collections.)

### transformers and actions on Spark

#### Transformations

Rerurn new *RDDs* as results.

**lazy: ** their result RDD is *not immediately* computed.

#### Actions

Compute result based on an RDD, and either returned or saved to an external storage system.

**eager: ** their result is *immediately* computed. 

**Laziness / Eageness is IMPORTANT**

#### Example 1

```scala
val largeList: List[String] = ...
val wordsRdd = sc.parallelize(largeList) // created RDD
val lengthsRdd = wordsRdd.map(_.length) // nothing is done [laginess]
```

```scala
val largeList: List[String] = ...
val wordsRdd = sc.parallelize(largeList) // created RDD
val lengthRDD = wordsRdd.map(_.length) // deferred
val totalChars = lengthRdd.reduce(_ + _) // lengthRDD created!!
```

#### Example 2

Determine the number of errors that were logged in December 2016.

Assuming that dates come in the form, `YYYY-MM-DD:HH:MM:SS`, and errors are loged with aprefix that includes the word “error”.

```scala
val lastYearsLogs: RDD[String] = ...
val numDecErrorLogs = lastYearsLogs.filter(lg => lg.contains("2016-12") && lg.contains("error")).count() // filter is not executed until count execute
```

## Iteration

### Example: Logistic Regression

The classifier’s weights are iteratively updated based on a training dataset.
$$
w\leftarrow w-\alpha\cdot\sum_{i=1}^{n}g(w;,x_i,y_i)
$$

```scala
val points = sc.textFile(...).map(parsePoint)
var w = Vector.zeros(d)
for (i <- 1 to numIterations) {
    val gradient = points.map {p => 
        (1 / (1 + exp(-p.y * w.dot(p.x))) - 1) * p.y * p.y
	}.reduce(_ + _)
    w -= alpha * gradient
}
```

`points` is being re-evaluated upon every iteration unnecessatily…

### Caching and Persistance

By default, RDDs are *recomputed each time you run an action* on them.

```scala
val lastYearsLogs: RDD[String] = ...
val logsWithErrors = lastYearsLogs.filter(_.contains("ERROR")).persist()
val firstLogsWithErrors = logsWithErrors.take(10)
val numErrors = logsWithErrors.count() // faster 
```

